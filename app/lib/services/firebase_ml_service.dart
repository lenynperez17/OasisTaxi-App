// ü§ñ Firebase ML Service - Sistema de Detecci√≥n de Toxicidad y Moderaci√≥n de Contenido
// Sistema completo de Machine Learning para OasisTaxi Peru

import 'dart:async';
import 'package:cloud_firestore/cloud_firestore.dart';
import 'package:firebase_auth/firebase_auth.dart';
import '../utils/app_logger.dart';

/// Resultado de an√°lisis de toxicidad
class ToxicityAnalysisResult {
  final String messageId;
  final String content;
  final bool isToxic;
  final double toxicityScore;
  final List<String> detectedCategories;
  final SentimentAnalysis sentiment;
  final DateTime analyzedAt;
  final String language;
  final ModerationType moderationType;

  const ToxicityAnalysisResult({
    required this.messageId,
    required this.content,
    required this.isToxic,
    required this.toxicityScore,
    required this.detectedCategories,
    required this.sentiment,
    required this.analyzedAt,
    required this.language,
    required this.moderationType,
  });

  Map<String, dynamic> toMap() {
    return {
      'messageId': messageId,
      'content': content,
      'isToxic': isToxic,
      'toxicityScore': toxicityScore,
      'detectedCategories': detectedCategories,
      'sentiment': sentiment.toMap(),
      'analyzedAt': Timestamp.fromDate(analyzedAt),
      'language': language,
      'moderationType': moderationType.toString(),
    };
  }

  factory ToxicityAnalysisResult.fromMap(Map<String, dynamic> map) {
    return ToxicityAnalysisResult(
      messageId: map['messageId'] ?? '',
      content: map['content'] ?? '',
      isToxic: map['isToxic'] ?? false,
      toxicityScore: (map['toxicityScore'] ?? 0.0).toDouble(),
      detectedCategories: List<String>.from(map['detectedCategories'] ?? []),
      sentiment: SentimentAnalysis.fromMap(map['sentiment'] ?? {}),
      analyzedAt: (map['analyzedAt'] as Timestamp?)?.toDate() ?? DateTime.now(),
      language: map['language'] ?? 'es',
      moderationType: ModerationType.values.firstWhere(
        (e) => e.toString() == map['moderationType'],
        orElse: () => ModerationType.automatic,
      ),
    );
  }
}

/// An√°lisis de sentimientos
class SentimentAnalysis {
  final double positiveScore;
  final double negativeScore;
  final double neutralScore;
  final SentimentLabel label;
  final double confidence;

  const SentimentAnalysis({
    required this.positiveScore,
    required this.negativeScore,
    required this.neutralScore,
    required this.label,
    required this.confidence,
  });

  Map<String, dynamic> toMap() {
    return {
      'positiveScore': positiveScore,
      'negativeScore': negativeScore,
      'neutralScore': neutralScore,
      'label': label.toString(),
      'confidence': confidence,
    };
  }

  factory SentimentAnalysis.fromMap(Map<String, dynamic> map) {
    return SentimentAnalysis(
      positiveScore: (map['positiveScore'] ?? 0.0).toDouble(),
      negativeScore: (map['negativeScore'] ?? 0.0).toDouble(),
      neutralScore: (map['neutralScore'] ?? 0.0).toDouble(),
      label: SentimentLabel.values.firstWhere(
        (e) => e.toString() == map['label'],
        orElse: () => SentimentLabel.neutral,
      ),
      confidence: (map['confidence'] ?? 0.0).toDouble(),
    );
  }
}

/// Etiquetas de sentimiento
enum SentimentLabel { positive, negative, neutral }

/// Tipos de moderaci√≥n
enum ModerationType { automatic, manual, hybrid }

/// Categor√≠as de contenido t√≥xico
enum ToxicityCategory {
  harassment,
  hateSpeech,
  violence,
  sexualContent,
  profanity,
  threat,
  spam,
  discrimination,
  bullying,
  extremism
}

/// Resultado de moderaci√≥n
class ModerationResult {
  final String id;
  final String userId;
  final String content;
  final ModerationAction action;
  final String reason;
  final DateTime createdAt;
  final String? moderatorId;
  final Map<String, dynamic> metadata;

  const ModerationResult({
    required this.id,
    required this.userId,
    required this.content,
    required this.action,
    required this.reason,
    required this.createdAt,
    this.moderatorId,
    required this.metadata,
  });

  Map<String, dynamic> toMap() {
    return {
      'id': id,
      'userId': userId,
      'content': content,
      'action': action.toString(),
      'reason': reason,
      'createdAt': Timestamp.fromDate(createdAt),
      'moderatorId': moderatorId,
      'metadata': metadata,
    };
  }

  factory ModerationResult.fromMap(Map<String, dynamic> map) {
    return ModerationResult(
      id: map['id'] ?? '',
      userId: map['userId'] ?? '',
      content: map['content'] ?? '',
      action: ModerationAction.values.firstWhere(
        (e) => e.toString() == map['action'],
        orElse: () => ModerationAction.none,
      ),
      reason: map['reason'] ?? '',
      createdAt: (map['createdAt'] as Timestamp?)?.toDate() ?? DateTime.now(),
      moderatorId: map['moderatorId'],
      metadata: Map<String, dynamic>.from(map['metadata'] ?? {}),
    );
  }
}

/// Acciones de moderaci√≥n
enum ModerationAction {
  none,
  warn,
  hide,
  delete,
  shadowBan,
  suspend,
  ban,
  escalate
}

/// Resultado de procesamiento en lote
class BatchProcessingResult {
  final int totalProcessed;
  final int toxicDetected;
  final int actionsApplied;
  final Duration processingTime;
  final List<ToxicityAnalysisResult> results;
  final List<String> errors;

  const BatchProcessingResult({
    required this.totalProcessed,
    required this.toxicDetected,
    required this.actionsApplied,
    required this.processingTime,
    required this.results,
    required this.errors,
  });

  Map<String, dynamic> toMap() {
    return {
      'totalProcessed': totalProcessed,
      'toxicDetected': toxicDetected,
      'actionsApplied': actionsApplied,
      'processingTimeMs': processingTime.inMilliseconds,
      'results': results.map((r) => r.toMap()).toList(),
      'errors': errors,
    };
  }
}

/// Configuraci√≥n de umbral de toxicidad
class ToxicityThreshold {
  final double lowThreshold;
  final double mediumThreshold;
  final double highThreshold;
  final Map<ToxicityCategory, double> categoryThresholds;

  const ToxicityThreshold({
    this.lowThreshold = 0.3,
    this.mediumThreshold = 0.6,
    this.highThreshold = 0.8,
    required this.categoryThresholds,
  });

  Map<String, dynamic> toMap() {
    return {
      'lowThreshold': lowThreshold,
      'mediumThreshold': mediumThreshold,
      'highThreshold': highThreshold,
      'categoryThresholds': categoryThresholds.map(
        (key, value) => MapEntry(key.toString(), value),
      ),
    };
  }

  factory ToxicityThreshold.defaultThreshold() {
    return ToxicityThreshold(
      categoryThresholds: {
        ToxicityCategory.harassment: 0.7,
        ToxicityCategory.hateSpeech: 0.8,
        ToxicityCategory.violence: 0.9,
        ToxicityCategory.sexualContent: 0.6,
        ToxicityCategory.profanity: 0.5,
        ToxicityCategory.threat: 0.9,
        ToxicityCategory.spam: 0.4,
        ToxicityCategory.discrimination: 0.8,
        ToxicityCategory.bullying: 0.7,
        ToxicityCategory.extremism: 0.9,
      },
    );
  }
}

/// Reportes de moderaci√≥n
class ModerationReport {
  final String reportId;
  final DateTime reportDate;
  final int totalMessages;
  final int toxicMessages;
  final int actionsApplied;
  final Map<String, int> actionBreakdown;
  final Map<String, int> categoryBreakdown;
  final List<String> topOffenders;
  final double averageToxicityScore;

  const ModerationReport({
    required this.reportId,
    required this.reportDate,
    required this.totalMessages,
    required this.toxicMessages,
    required this.actionsApplied,
    required this.actionBreakdown,
    required this.categoryBreakdown,
    required this.topOffenders,
    required this.averageToxicityScore,
  });

  Map<String, dynamic> toMap() {
    return {
      'reportId': reportId,
      'reportDate': Timestamp.fromDate(reportDate),
      'totalMessages': totalMessages,
      'toxicMessages': toxicMessages,
      'actionsApplied': actionsApplied,
      'actionBreakdown': actionBreakdown,
      'categoryBreakdown': categoryBreakdown,
      'topOffenders': topOffenders,
      'averageToxicityScore': averageToxicityScore,
    };
  }
}

/// ü§ñ Firebase ML Service - Servicio Principal
///
/// Servicio singleton que maneja toda la funcionalidad de Machine Learning
/// para detecci√≥n de toxicidad, an√°lisis de sentimientos y moderaci√≥n autom√°tica
///
/// Funcionalidades:
/// - Detecci√≥n de contenido t√≥xico en tiempo real
/// - An√°lisis de sentimientos
/// - Moderaci√≥n autom√°tica con escalamiento
/// - Soporte para espa√±ol y quechua
/// - Procesamiento en lote
/// - Reportes y analytics
/// - Integraci√≥n con Cloud Functions
/// - Cache distribuido con Redis
/// - Auditor√≠a completa
class FirebaseMLService {
  static final FirebaseMLService _instance = FirebaseMLService._internal();
  factory FirebaseMLService() => _instance;
  FirebaseMLService._internal();

  // Instancias de servicios
  final FirebaseFirestore _firestore = FirebaseFirestore.instance;
  final FirebaseAuth _auth = FirebaseAuth.instance;

  // Configuraciones
  ToxicityThreshold _threshold = ToxicityThreshold.defaultThreshold();
  bool _isInitialized = false;

  // Cache y m√©tricas
  final Map<String, ToxicityAnalysisResult> _analysisCache = {};
  final Map<String, int> _userViolationCount = {};
  Timer? _cacheCleanupTimer;

  // Listas de palabras prohibidas espec√≠ficas para Per√∫
  final Set<String> _spanishProfanity = {
    'mierda', 'carajo', 'pendejo', 'huev√≥n', 'conchatumadre', 'maric√≥n',
    'puta', 'joder', 'co√±o', 'idiota', 'est√∫pido', 'imb√©cil', 'tarado',
    'serrano', 'cholo',
    'indio', // T√©rminos discriminatorios espec√≠ficos de Per√∫
  };

  final Set<String> _quechuaProfanity = {
    'huk\'ucha', 'q\'ala', 'mana allin', // Palabras ofensivas en quechua
  };

  final Set<String> _threatWords = {
    'matar',
    'asesinar',
    'golpear',
    'romper',
    'destruir',
    'lastimar',
    'da√±ar',
    'herir',
    'amenazar',
    'secuestrar',
    'robar',
    'violar'
  };

  /// Inicializar el servicio
  Future<bool> initialize() async {
    try {
      if (_isInitialized) return true;

      AppLogger.info('ü§ñ Inicializando Firebase ML Service...');

      // Cargar configuraciones desde Firestore
      await _loadMLConfiguration();

      // Inicializar cache cleanup timer
      _startCacheCleanup();

      _isInitialized = true;
      AppLogger.info('‚úÖ Firebase ML Service inicializado correctamente');

      return true;
    } catch (e, stackTrace) {
      AppLogger.error(
          '‚ùå Error inicializando Firebase ML Service', e, stackTrace);
      return false;
    }
  }

  /// Cargar configuraci√≥n de ML desde Firestore
  Future<void> _loadMLConfiguration() async {
    try {
      final configDoc = await _firestore
          .collection('ml_configuration')
          .doc('toxicity_settings')
          .get();

      if (configDoc.exists) {
        final data = configDoc.data()!;
        _threshold = ToxicityThreshold(
          lowThreshold: (data['lowThreshold'] ?? 0.3).toDouble(),
          mediumThreshold: (data['mediumThreshold'] ?? 0.6).toDouble(),
          highThreshold: (data['highThreshold'] ?? 0.8).toDouble(),
          categoryThresholds: Map<ToxicityCategory, double>.fromEntries(
            ToxicityCategory.values.map((category) {
              final value = data['categoryThresholds']?[category.toString()];
              return MapEntry(category, (value ?? 0.7).toDouble());
            }),
          ),
        );
      }
    } catch (e) {
      AppLogger.warning(
          '‚ö†Ô∏è No se pudo cargar configuraci√≥n ML, usando defaults');
    }
  }

  /// Inicializar timer de limpieza de cache
  void _startCacheCleanup() {
    _cacheCleanupTimer?.cancel();
    _cacheCleanupTimer = Timer.periodic(
      const Duration(hours: 1),
      (timer) => _cleanupCache(),
    );
  }

  /// Limpiar cache expirado
  void _cleanupCache() {
    final now = DateTime.now();
    _analysisCache.removeWhere((key, result) {
      return now.difference(result.analyzedAt).inHours > 24;
    });
    AppLogger.debug(
        'üßπ Cache ML limpiado: ${_analysisCache.length} items restantes');
  }


  /// üéØ AN√ÅLISIS PRINCIPAL DE TOXICIDAD
  ///
  /// Analiza un texto para detectar contenido t√≥xico usando m√∫ltiples m√©todos:
  /// 1. Filtro de palabras prohibidas local
  /// 2. An√°lisis con Google Cloud Natural Language API
  /// 3. An√°lisis de patrones espec√≠ficos para contexto peruano
  Future<ToxicityAnalysisResult> analyzeTextToxicity(
    String text, {
    String? messageId,
    String? userId,
    String language = 'es',
  }) async {
    try {
      final id = messageId ?? _generateId();
      final startTime = DateTime.now();

      AppLogger.info('üîç Analizando toxicidad para mensaje: $id');

      // 1. Verificar cache primero
      final cacheKey = '${text.hashCode}_$language';
      if (_analysisCache.containsKey(cacheKey)) {
        final cached = _analysisCache[cacheKey]!;
        AppLogger.debug('üì¶ Resultado desde cache para: $id');
        return cached;
      }

      // 2. Pre-procesamiento del texto
      final cleanText = _preprocessText(text);

      // 3. An√°lisis local r√°pido
      final localResult =
          await _performLocalToxicityAnalysis(cleanText, language);

      // 4. An√°lisis con Cloud APIs (si el local no detecta toxicidad alta)
      ToxicityAnalysisResult finalResult;
      if (localResult.toxicityScore < _threshold.mediumThreshold) {
        finalResult = await _performCloudToxicityAnalysis(
          cleanText,
          id,
          language,
          localResult,
        );
      } else {
        finalResult = localResult;
      }

      // 5. An√°lisis de sentimientos
      final sentiment = await _performSentimentAnalysis(cleanText, language);
      finalResult = ToxicityAnalysisResult(
        messageId: finalResult.messageId,
        content: finalResult.content,
        isToxic: finalResult.isToxic,
        toxicityScore: finalResult.toxicityScore,
        detectedCategories: finalResult.detectedCategories,
        sentiment: sentiment,
        analyzedAt: finalResult.analyzedAt,
        language: finalResult.language,
        moderationType: finalResult.moderationType,
      );

      // 6. Guardar en cache y Firestore
      _analysisCache[cacheKey] = finalResult;
      await _saveToxicityAnalysis(finalResult, userId);

      // 7. Aplicar moderaci√≥n autom√°tica si es necesario
      if (finalResult.isToxic && userId != null) {
        await _applyAutoModeration(finalResult, userId);
      }

      final duration = DateTime.now().difference(startTime);
      AppLogger.performance(
          'ü§ñ An√°lisis ML completado', duration.inMilliseconds);

      return finalResult;
    } catch (e, stackTrace) {
      AppLogger.error('‚ùå Error en an√°lisis de toxicidad', e, stackTrace);
      return _createErrorResult(text, messageId ?? _generateId());
    }
  }

  /// Pre-procesar texto para an√°lisis
  String _preprocessText(String text) {
    return text
        .toLowerCase()
        .trim()
        .replaceAll(
            RegExp(r'[^\w\s\u00C0-\u017F]'), ' ') // Mantener caracteres latinos
        .replaceAll(RegExp(r'\s+'), ' ');
  }

  /// An√°lisis local de toxicidad (r√°pido)
  Future<ToxicityAnalysisResult> _performLocalToxicityAnalysis(
    String text,
    String language,
  ) async {
    double toxicityScore = 0.0;
    final List<String> detectedCategories = [];

    // Verificar palabras prohibidas
    final words = text.split(' ');
    int profanityCount = 0;
    int threatCount = 0;

    for (final word in words) {
      if (_spanishProfanity.contains(word) ||
          (language == 'qu' && _quechuaProfanity.contains(word))) {
        profanityCount++;
        detectedCategories.add(ToxicityCategory.profanity.toString());
      }

      if (_threatWords.contains(word)) {
        threatCount++;
        detectedCategories.add(ToxicityCategory.threat.toString());
      }
    }

    // Calcular score basado en densidad de palabras t√≥xicas
    final totalWords = words.length;
    if (totalWords > 0) {
      final profanityRatio = profanityCount / totalWords;
      final threatRatio = threatCount / totalWords;

      toxicityScore = (profanityRatio * 0.6) + (threatRatio * 0.9);

      // Ajustes por contexto
      if (text.contains('uber') || text.contains('taxi')) {
        toxicityScore *=
            0.8; // Reducir falsos positivos en contexto de transporte
      }
    }

    // Verificar patrones espec√≠ficos
    if (_containsDiscriminatoryContent(text)) {
      toxicityScore = (toxicityScore + 0.7).clamp(0.0, 1.0);
      detectedCategories.add(ToxicityCategory.discrimination.toString());
    }

    return ToxicityAnalysisResult(
      messageId: _generateId(),
      content: text,
      isToxic: toxicityScore > _threshold.lowThreshold,
      toxicityScore: toxicityScore,
      detectedCategories: detectedCategories.toSet().toList(),
      sentiment: const SentimentAnalysis(
        positiveScore: 0.0,
        negativeScore: 0.0,
        neutralScore: 1.0,
        label: SentimentLabel.neutral,
        confidence: 0.5,
      ),
      analyzedAt: DateTime.now(),
      language: language,
      moderationType: ModerationType.automatic,
    );
  }

  /// Verificar contenido discriminatorio espec√≠fico para Per√∫
  bool _containsDiscriminatoryContent(String text) {
    final discriminatoryPatterns = [
      r'\bserrano\b.*\bmalo\b',
      r'\bcholo\b.*\binferior\b',
      r'\bindio\b.*\best√∫pido\b',
      r'\bcosta\b.*\bmejor.*\bsierra\b',
      r'\blima\b.*\bmejor.*\bprovinc\w+',
    ];

    for (final pattern in discriminatoryPatterns) {
      if (RegExp(pattern, caseSensitive: false).hasMatch(text)) {
        return true;
      }
    }

    return false;
  }

  /// An√°lisis con Cloud APIs (m√°s preciso pero m√°s lento)
  Future<ToxicityAnalysisResult> _performCloudToxicityAnalysis(
    String text,
    String messageId,
    String language,
    ToxicityAnalysisResult localResult,
  ) async {
    try {
      // En un entorno real, aqu√≠ se llamar√≠a a Google Cloud Natural Language API
      // Por ahora, simularemos la respuesta basada en el an√°lisis local

      // Combinar resultado local con an√°lisis m√°s sofisticado
      double enhancedScore = localResult.toxicityScore;
      final enhancedCategories =
          List<String>.from(localResult.detectedCategories);

      // An√°lisis de patrones m√°s complejos
      if (_containsHarassmentPatterns(text)) {
        enhancedScore = (enhancedScore + 0.6).clamp(0.0, 1.0);
        enhancedCategories.add(ToxicityCategory.harassment.toString());
      }

      if (_containsHateSpeechPatterns(text)) {
        enhancedScore = (enhancedScore + 0.8).clamp(0.0, 1.0);
        enhancedCategories.add(ToxicityCategory.hateSpeech.toString());
      }

      return ToxicityAnalysisResult(
        messageId: messageId,
        content: text,
        isToxic: enhancedScore > _threshold.lowThreshold,
        toxicityScore: enhancedScore,
        detectedCategories: enhancedCategories.toSet().toList(),
        sentiment: localResult.sentiment,
        analyzedAt: DateTime.now(),
        language: language,
        moderationType: ModerationType.hybrid,
      );
    } catch (e) {
      AppLogger.error('‚ùå Error en an√°lisis Cloud ML', e);
      return localResult; // Fallback al resultado local
    }
  }

  /// Detectar patrones de acoso
  bool _containsHarassmentPatterns(String text) {
    final harassmentPatterns = [
      r'\bno.*mereces.*trabajo\b',
      r'\bdeber√≠as.*dejar.*manejar\b',
      r'\bmujeres.*no.*saben.*conducir\b',
      r'\beres.*muy.*feo\b',
      r'\bno.*sirves.*para.*nada\b',
    ];

    return harassmentPatterns
        .any((pattern) => RegExp(pattern, caseSensitive: false).hasMatch(text));
  }

  /// Detectar patrones de discurso de odio
  bool _containsHateSpeechPatterns(String text) {
    final hateSpeechPatterns = [
      r'\btodos.*los.*\w+.*son.*\w+',
      r'\bodio.*a.*los.*\w+',
      r'\bdeber√≠an.*matar.*a.*\w+',
      r'\bno.*merecen.*vivir\b',
    ];

    return hateSpeechPatterns
        .any((pattern) => RegExp(pattern, caseSensitive: false).hasMatch(text));
  }

  /// An√°lisis de sentimientos
  Future<SentimentAnalysis> _performSentimentAnalysis(
    String text,
    String language,
  ) async {
    try {
      // An√°lisis b√°sico de sentimientos
      double positiveScore = 0.0;
      double negativeScore = 0.0;
      double neutralScore = 0.5;

      final positiveWords = language == 'es'
          ? [
              'bueno',
              'excelente',
              'genial',
              'perfecto',
              'gracias',
              'amable',
              'r√°pido'
            ]
          : ['allin', 'sumaq', 'kusay']; // Quechua

      final negativeWords = language == 'es'
          ? [
              'malo',
              'terrible',
              'p√©simo',
              'horrible',
              'lento',
              'grosero',
              'sucio'
            ]
          : ['mana allin', 'millay', 'q\'ala']; // Quechua

      final words = text.split(' ');
      int positiveCount = 0;
      int negativeCount = 0;

      for (final word in words) {
        if (positiveWords.contains(word)) positiveCount++;
        if (negativeWords.contains(word)) negativeCount++;
      }

      if (words.isNotEmpty) {
        positiveScore = positiveCount / words.length;
        negativeScore = negativeCount / words.length;
        neutralScore = 1.0 - (positiveScore + negativeScore);
      }

      SentimentLabel label;
      double confidence;

      if (positiveScore > negativeScore) {
        label = SentimentLabel.positive;
        confidence = positiveScore;
      } else if (negativeScore > positiveScore) {
        label = SentimentLabel.negative;
        confidence = negativeScore;
      } else {
        label = SentimentLabel.neutral;
        confidence = neutralScore;
      }

      return SentimentAnalysis(
        positiveScore: positiveScore,
        negativeScore: negativeScore,
        neutralScore: neutralScore,
        label: label,
        confidence: confidence,
      );
    } catch (e) {
      AppLogger.error('‚ùå Error en an√°lisis de sentimientos', e);
      return const SentimentAnalysis(
        positiveScore: 0.0,
        negativeScore: 0.0,
        neutralScore: 1.0,
        label: SentimentLabel.neutral,
        confidence: 0.5,
      );
    }
  }

  /// Guardar an√°lisis en Firestore
  Future<void> _saveToxicityAnalysis(
    ToxicityAnalysisResult result,
    String? userId,
  ) async {
    try {
      final docData = result.toMap();
      if (userId != null) {
        docData['userId'] = userId;
      }

      await _firestore
          .collection('ml_toxicity_analysis')
          .doc(result.messageId)
          .set(docData);

      AppLogger.firebase('üíæ An√°lisis ML guardado: ${result.messageId}');
    } catch (e) {
      AppLogger.error('‚ùå Error guardando an√°lisis ML', e);
    }
  }

  /// Aplicar moderaci√≥n autom√°tica
  Future<void> _applyAutoModeration(
    ToxicityAnalysisResult result,
    String userId,
  ) async {
    try {
      // Incrementar contador de violaciones del usuario
      _userViolationCount[userId] = (_userViolationCount[userId] ?? 0) + 1;
      final violationCount = _userViolationCount[userId]!;

      ModerationAction action;
      String reason;

      // Determinar acci√≥n basada en score y historial
      if (result.toxicityScore >= _threshold.highThreshold ||
          violationCount >= 5) {
        action = violationCount >= 10
            ? ModerationAction.ban
            : ModerationAction.suspend;
        reason = 'Contenido altamente t√≥xico detectado autom√°ticamente';
      } else if (result.toxicityScore >= _threshold.mediumThreshold ||
          violationCount >= 3) {
        action = ModerationAction.shadowBan;
        reason = 'M√∫ltiples violaciones detectadas';
      } else {
        action = ModerationAction.warn;
        reason = 'Contenido potencialmente inapropiado';
      }

      final moderationResult = ModerationResult(
        id: _generateId(),
        userId: userId,
        content: result.content,
        action: action,
        reason: reason,
        createdAt: DateTime.now(),
        metadata: {
          'toxicityScore': result.toxicityScore,
          'categories': result.detectedCategories,
          'violationCount': violationCount,
          'automatic': true,
        },
      );

      // Guardar resultado de moderaci√≥n
      await _firestore
          .collection('moderation_actions')
          .doc(moderationResult.id)
          .set(moderationResult.toMap());

      // Actualizar contador de usuario en Firestore
      await _firestore.collection('user_violations').doc(userId).set({
        'count': violationCount,
        'lastViolation': Timestamp.fromDate(DateTime.now()),
        'actions': FieldValue.arrayUnion([moderationResult.id]),
      }, SetOptions(merge: true));

      AppLogger.info(
          '‚öñÔ∏è Acci√≥n autom√°tica aplicada: $action para usuario $userId');

      // Escalar a revisi√≥n manual si es muy grave
      if (action == ModerationAction.ban || result.toxicityScore >= 0.9) {
        await _escalateToHumanReview(result, moderationResult);
      }
    } catch (e, stackTrace) {
      AppLogger.error('‚ùå Error aplicando moderaci√≥n autom√°tica', e, stackTrace);
    }
  }

  /// Escalar a revisi√≥n humana
  Future<void> _escalateToHumanReview(
    ToxicityAnalysisResult analysis,
    ModerationResult moderation,
  ) async {
    try {
      await _firestore.collection('human_review_queue').doc(_generateId()).set({
        'analysisId': analysis.messageId,
        'moderationId': moderation.id,
        'priority': analysis.toxicityScore >= 0.9 ? 'high' : 'medium',
        'createdAt': Timestamp.fromDate(DateTime.now()),
        'status': 'pending',
        'content': analysis.content,
        'userId': moderation.userId,
        'toxicityScore': analysis.toxicityScore,
        'categories': analysis.detectedCategories,
      });

      AppLogger.info('üö® Escalado a revisi√≥n humana: ${analysis.messageId}');
    } catch (e) {
      AppLogger.error('‚ùå Error escalando a revisi√≥n humana', e);
    }
  }

  /// üìä PROCESAMIENTO EN LOTE
  ///
  /// Procesar m√∫ltiples mensajes en lote para an√°lisis masivo
  Future<BatchProcessingResult> processBatch(
    List<Map<String, String>> messages, {
    int batchSize = 50,
  }) async {
    final startTime = DateTime.now();
    final List<ToxicityAnalysisResult> results = [];
    final List<String> errors = [];
    int toxicDetected = 0;
    int actionsApplied = 0;

    AppLogger.info(
        'üì¶ Iniciando procesamiento en lote: ${messages.length} mensajes');

    try {
      // Procesar en chunks para evitar sobrecarga
      for (int i = 0; i < messages.length; i += batchSize) {
        final chunk = messages.skip(i).take(batchSize).toList();

        final futures = chunk.map((msg) async {
          try {
            final result = await analyzeTextToxicity(
              msg['content'] ?? '',
              messageId: msg['id'],
              userId: msg['userId'],
              language: msg['language'] ?? 'es',
            );

            if (result.isToxic) {
              toxicDetected++;
              if (msg['userId'] != null) {
                actionsApplied++;
              }
            }

            return result;
          } catch (e) {
            errors.add('Error procesando mensaje ${msg['id']}: $e');
            return null;
          }
        });

        final chunkResults = await Future.wait(futures);
        results.addAll(chunkResults.whereType<ToxicityAnalysisResult>());

        // Pausa breve entre chunks para evitar rate limiting
        if (i + batchSize < messages.length) {
          await Future.delayed(const Duration(milliseconds: 100));
        }
      }

      final processingTime = DateTime.now().difference(startTime);
      AppLogger.performance(
          'üì¶ Procesamiento en lote completado', processingTime.inMilliseconds);

      return BatchProcessingResult(
        totalProcessed: messages.length,
        toxicDetected: toxicDetected,
        actionsApplied: actionsApplied,
        processingTime: processingTime,
        results: results,
        errors: errors,
      );
    } catch (e, stackTrace) {
      AppLogger.error('‚ùå Error en procesamiento en lote', e, stackTrace);
      throw Exception('Error en procesamiento en lote: $e');
    }
  }

  /// üìà REPORTES Y ANALYTICS
  ///
  /// Generar reporte de moderaci√≥n para un per√≠odo espec√≠fico
  Future<ModerationReport> generateModerationReport({
    DateTime? startDate,
    DateTime? endDate,
  }) async {
    try {
      startDate ??= DateTime.now().subtract(const Duration(days: 7));
      endDate ??= DateTime.now();

      AppLogger.info(
          'üìà Generando reporte de moderaci√≥n: ${startDate.toString().split(' ')[0]} - ${endDate.toString().split(' ')[0]}');

      // Consultar an√°lisis del per√≠odo
      final analysisQuery = await _firestore
          .collection('ml_toxicity_analysis')
          .where('analyzedAt',
              isGreaterThanOrEqualTo: Timestamp.fromDate(startDate))
          .where('analyzedAt', isLessThanOrEqualTo: Timestamp.fromDate(endDate))
          .get();

      // Consultar acciones de moderaci√≥n
      final moderationQuery = await _firestore
          .collection('moderation_actions')
          .where('createdAt',
              isGreaterThanOrEqualTo: Timestamp.fromDate(startDate))
          .where('createdAt', isLessThanOrEqualTo: Timestamp.fromDate(endDate))
          .get();

      // Procesar estad√≠sticas
      int totalMessages = analysisQuery.docs.length;
      int toxicMessages = 0;
      double totalToxicityScore = 0.0;
      final Map<String, int> categoryBreakdown = {};
      final Map<String, int> actionBreakdown = {};
      final Map<String, int> userViolations = {};

      // Analizar resultados de toxicidad
      for (final doc in analysisQuery.docs) {
        final data = doc.data();
        final isToxic = data['isToxic'] ?? false;
        final score = (data['toxicityScore'] ?? 0.0).toDouble();
        final categories = List<String>.from(data['detectedCategories'] ?? []);
        final userId = data['userId'];

        if (isToxic) {
          toxicMessages++;
          totalToxicityScore += score;

          if (userId != null) {
            userViolations[userId] = (userViolations[userId] ?? 0) + 1;
          }
        }

        for (final category in categories) {
          categoryBreakdown[category] = (categoryBreakdown[category] ?? 0) + 1;
        }
      }

      // Analizar acciones de moderaci√≥n
      for (final doc in moderationQuery.docs) {
        final data = doc.data();
        final action = data['action'] ?? 'none';
        actionBreakdown[action] = (actionBreakdown[action] ?? 0) + 1;
      }

      // Top ofensores
      final violationsList = userViolations.entries.toList();
      violationsList.sort((a, b) => b.value.compareTo(a.value));
      final topOffenders = violationsList.take(10).map((e) => e.key).toList();

      final averageToxicityScore =
          toxicMessages > 0 ? totalToxicityScore / toxicMessages : 0.0;

      final report = ModerationReport(
        reportId: _generateId(),
        reportDate: DateTime.now(),
        totalMessages: totalMessages,
        toxicMessages: toxicMessages,
        actionsApplied: moderationQuery.docs.length,
        actionBreakdown: actionBreakdown,
        categoryBreakdown: categoryBreakdown,
        topOffenders: topOffenders,
        averageToxicityScore: averageToxicityScore,
      );

      // Guardar reporte
      await _firestore
          .collection('moderation_reports')
          .doc(report.reportId)
          .set(report.toMap());

      AppLogger.info('üìä Reporte generado: ${report.reportId}');
      return report;
    } catch (e, stackTrace) {
      AppLogger.error('‚ùå Error generando reporte', e, stackTrace);
      throw Exception('Error generando reporte de moderaci√≥n: $e');
    }
  }

  /// üîÑ MODERACI√ìN MANUAL
  ///
  /// Obtener cola de revisi√≥n humana
  Future<List<Map<String, dynamic>>> getHumanReviewQueue({
    String priority = 'all',
    int limit = 50,
  }) async {
    try {
      Query query = _firestore
          .collection('human_review_queue')
          .where('status', isEqualTo: 'pending')
          .orderBy('createdAt', descending: true)
          .limit(limit);

      if (priority != 'all') {
        query = query.where('priority', isEqualTo: priority);
      }

      final result = await query.get();
      return result.docs
          .map((doc) => {
                'id': doc.id,
                ...doc.data() as Map<String, dynamic>,
              })
          .toList();
    } catch (e) {
      AppLogger.error('‚ùå Error obteniendo cola de revisi√≥n', e);
      return [];
    }
  }

  /// Aprobar/rechazar revisi√≥n manual
  Future<bool> processHumanReview(
    String reviewId,
    bool approved,
    String moderatorId, {
    String? notes,
  }) async {
    try {
      await _firestore.collection('human_review_queue').doc(reviewId).update({
        'status': approved ? 'approved' : 'rejected',
        'moderatorId': moderatorId,
        'processedAt': Timestamp.fromDate(DateTime.now()),
        'notes': notes ?? '',
      });

      AppLogger.info(
          '‚úÖ Revisi√≥n humana procesada: $reviewId - ${approved ? 'Aprobada' : 'Rechazada'}');
      return true;
    } catch (e) {
      AppLogger.error('‚ùå Error procesando revisi√≥n humana', e);
      return false;
    }
  }

  /// üîß UTILIDADES AUXILIARES

  /// Crear resultado de error
  ToxicityAnalysisResult _createErrorResult(String content, String messageId) {
    return ToxicityAnalysisResult(
      messageId: messageId,
      content: content,
      isToxic: false,
      toxicityScore: 0.0,
      detectedCategories: [],
      sentiment: const SentimentAnalysis(
        positiveScore: 0.0,
        negativeScore: 0.0,
        neutralScore: 1.0,
        label: SentimentLabel.neutral,
        confidence: 0.0,
      ),
      analyzedAt: DateTime.now(),
      language: 'es',
      moderationType: ModerationType.automatic,
    );
  }

  /// Generar ID √∫nico
  String _generateId() {
    return DateTime.now().millisecondsSinceEpoch.toString() +
        (1000 + DateTime.now().microsecond % 9000).toString();
  }

  /// Obtener estad√≠sticas del usuario
  Future<Map<String, dynamic>> getUserModerationStats(String userId) async {
    try {
      final violationsDoc =
          await _firestore.collection('user_violations').doc(userId).get();

      if (!violationsDoc.exists) {
        return {
          'violationCount': 0,
          'lastViolation': null,
          'status': 'good',
        };
      }

      final data = violationsDoc.data()!;
      final count = data['count'] ?? 0;
      final lastViolation = (data['lastViolation'] as Timestamp?)?.toDate();

      String status;
      if (count == 0) {
        status = 'good';
      } else if (count < 3) {
        status = 'warning';
      } else if (count < 10) {
        status = 'restricted';
      } else {
        status = 'banned';
      }

      return {
        'violationCount': count,
        'lastViolation': lastViolation,
        'status': status,
        'actions': List<String>.from(data['actions'] ?? []),
      };
    } catch (e) {
      AppLogger.error('‚ùå Error obteniendo stats de usuario', e);
      return {
        'violationCount': 0,
        'lastViolation': null,
        'status': 'unknown',
      };
    }
  }

  /// Limpiar recursos
  void dispose() {
    _cacheCleanupTimer?.cancel();
    _analysisCache.clear();
    _userViolationCount.clear();
    _isInitialized = false;
    AppLogger.info('üßπ Firebase ML Service recursos limpiados');
  }

  /// Obtener estado del servicio
  Map<String, dynamic> getServiceStatus() {
    return {
      'isInitialized': _isInitialized,
      'cacheSize': _analysisCache.length,
      'userViolationsCached': _userViolationCount.length,
      'thresholds': _threshold.toMap(),
      'uptime': DateTime.now().toIso8601String(),
    };
  }
}

/// üöÄ Extensiones auxiliares para facilitar el uso

extension FirebaseMLServiceExtensions on FirebaseMLService {
  /// An√°lisis r√°pido para mensajes de chat
  Future<bool> isChatMessageSafe(String message, String userId) async {
    final result = await analyzeTextToxicity(
      message,
      userId: userId,
      language: 'es',
    );
    return !result.isToxic;
  }

  /// An√°lisis de comentarios de calificaci√≥n
  Future<ToxicityAnalysisResult> analyzeRatingComment(
    String comment,
    String tripId,
    String userId,
  ) async {
    return await analyzeTextToxicity(
      comment,
      messageId: '${tripId}_rating_$userId',
      userId: userId,
      language: 'es',
    );
  }

  /// Verificar si usuario puede enviar mensajes
  Future<bool> canUserSendMessages(String userId) async {
    final stats = await getUserModerationStats(userId);
    return stats['status'] != 'banned' && stats['status'] != 'restricted';
  }

  /// Analizar contexto del viaje para detecci√≥n de patrones sospechosos
  Future<Map<String, dynamic>> analyzeTripContext({
    required String userId,
    required String rideId,
    required Map<String, dynamic> rideDetails,
    Map<String, dynamic>? userHistory,
  }) async {
    try {
      AppLogger.info('üîç Analizando contexto del viaje: $rideId');

      // An√°lisis b√°sico del contexto del viaje
      final analysis = <String, dynamic>{
        'rideId': rideId,
        'userId': userId,
        'timestamp': DateTime.now().toIso8601String(),
        'suspiciousPatterns': <String>[],
        'riskScore': 0.0,
        'recommendations': <String>[],
      };

      // Analizar patrones de precio
      if (rideDetails['price'] != null) {
        final price = (rideDetails['price'] as num).toDouble();
        if (price > 100.0) {
          // Precio inusualmente alto
          analysis['suspiciousPatterns'].add('high_price');
          analysis['riskScore'] = (analysis['riskScore'] as double) + 0.3;
        }
      }

      // Analizar distancia/tiempo
      if (rideDetails['estimatedDistance'] != null) {
        final distance = (rideDetails['estimatedDistance'] as num).toDouble();
        if (distance > 50.0) {
          // Viaje muy largo
          analysis['suspiciousPatterns'].add('long_distance');
          analysis['riskScore'] = (analysis['riskScore'] as double) + 0.2;
        }
      }

      // Analizar horario (viajes nocturnos)
      final currentHour = DateTime.now().hour;
      if (currentHour >= 23 || currentHour <= 5) {
        analysis['suspiciousPatterns'].add('late_night_trip');
        analysis['riskScore'] = (analysis['riskScore'] as double) + 0.1;
      }

      // Generar recomendaciones basadas en riesgo
      final riskScore = analysis['riskScore'] as double;
      if (riskScore > 0.5) {
        analysis['recommendations'].add('additional_verification_required');
      }
      if (riskScore > 0.7) {
        analysis['recommendations'].add('manual_review_required');
      }

      AppLogger.info(
          '‚úÖ An√°lisis de contexto completado - Riesgo: ${riskScore.toStringAsFixed(2)}');
      return analysis;
    } catch (e, stackTrace) {
      AppLogger.error('Error analizando contexto del viaje', e, stackTrace);
      return {
        'rideId': rideId,
        'userId': userId,
        'error': e.toString(),
        'riskScore': 0.0,
        'suspiciousPatterns': <String>[],
        'recommendations': <String>[],
      };
    }
  }
}
